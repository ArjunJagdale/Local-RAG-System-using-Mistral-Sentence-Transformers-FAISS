# ğŸ§  Local RAG System using Mistral + Sentence Transformers + FAISS

This project is a **lightweight, fully local Retrieval-Augmented Generation (RAG)** system using:

- ğŸ§© [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b) (run locally via `llama-cpp-python`)
- ğŸ” Sentence Transformers (`all-MiniLM-L6-v2`) for semantic chunk embeddings
- ğŸ§  FAISS for fast similarity search
- ğŸ“„ Support for both **file-based** and **web URL** context
- ğŸ’¬ Command-line chat interface with contextual answers

---
## Demo Video


https://github.com/user-attachments/assets/a880a5db-759d-42e4-8dd2-2091877b445d


---

## ğŸ” Retrieval-Augmented Generation with Mistral LLM
This project demonstrates how to build a lightweight, offline-capable RAG system using:

- ğŸ§  A local Mistral 7B LLM (GGUF format)

- ğŸ” Context retrieval using FAISS + Sentence Transformers

- ğŸŒ Optional web URL scraping or local text files

- ğŸ›  No Hugging Face APIs â€” 100% free and local

---

## ğŸ“¦ Features

- ğŸ”Œ No API keys or cloud dependencies â€“ everything runs **offline**
- ğŸŒ Ingest text from **URLs** (via web scraping) or **local `.txt` files**
- ğŸ” Efficient semantic search with FAISS
- ğŸ’¡ Powered by a local `GGUF` version of **Mistral 7B**

---

## ğŸš€ Installation

1. **Clone the repo**
```bash
git clone https://github.com/your-username/Local-RAG-System-using-Mistral-Sentence-Transformers-FAISS.git
cd local-mistral-rag
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Download a Mistral GGUF Model**

You can get one from Hugging Face:

- [TheBloke/Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)

Place it in your preferred folder and copy the path to the `.gguf` file.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ embedder.py             â† Embedding + FAISS index logic
â”‚   â”œâ”€â”€ file_loader.py          â† Load plain text files
â”‚   â”œâ”€â”€ web_scraper.py          â† Extract text from webpages
â”‚   â””â”€â”€ mistral_interface.py    â† Prompt Mistral using llama-cpp
â”œâ”€â”€ rag_main.py                 â† Main CLI script to run RAG system
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  Usage

### ğŸ“ Load from a local text file
```bash
python main.py --file path/to/text.txt --model_path path/to/mistral-model.gguf
```

### ğŸŒ Load from a web URL
```bash
python main.py --url "https://example.com/article" --model_path path/to/mistral-model.gguf
```

Then ask questions in the command line!

```txt
Ask a question (or type 'exit'): What is the main idea of the article?
```

---

## ğŸ›  Requirements

- Python 3.8+
- A system with 8GB+ RAM (for faster performance)
- `llama-cpp-python` with CPU or GPU support

---

## âš™ï¸ Advanced Tips

- You can increase the context window of Mistral up to **32768 tokens**:
```python
Llama(model_path=model_path, n_ctx=8192)
```

- To improve semantic matching, replace the embedder model with a larger SentenceTransformer if needed.

---

## âœ… Example Use Case

- Ask questions based on downloaded PDFs converted to text.
- Summarize large blog posts or research papers.
- Run it 100% offline for privacy-focused applications.

---

## ğŸ™Œ Credits

- [Mistral 7B](https://mistral.ai/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [SentenceTransformers](https://www.sbert.net/)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)

---
